{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a29a9410-6e29-4d7b-86aa-02b0f611f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e4c340-806d-4ce3-a755-525df492a09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"clarin-knext/fiqa-pl\", \"corpus\")\n",
    "ds_queries = load_dataset(\"clarin-knext/fiqa-pl\",\"queries\")\n",
    "ds_qa = load_dataset(\"clarin-knext/fiqa-pl-qrels\")['train'] # we will train the model on a qrels training subset\n",
    "\n",
    "model_name = 'allegro/herbert-base-cased'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b936168-c0d6-4ba4-b3dc-2df0b7a05823",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.DataFrame(ds['corpus'])\n",
    "queries = pd.DataFrame(ds_queries['queries'])\n",
    "qrels = pd.DataFrame(ds_qa)\n",
    "\n",
    "corpus.drop(columns=['title'], inplace=True)\n",
    "queries.drop(columns=['title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d07baf-696e-4d5b-8060-80487362bd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: a) corpus: 57638 b) queries: 6648 c) qrels (train subset): 14166\n",
      "<class 'numpy.int64'> <class 'str'> <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"rows: a) corpus: {len(corpus)} b) queries: {len(queries)} c) qrels (train subset): {len(qrels)}\")\n",
    "print(type(qrels['query-id'][0]), type(queries['_id'][0]), type(corpus['_id'][0]))\n",
    "queries['_id'] = queries['_id'].astype(int)\n",
    "corpus['_id'] = corpus['_id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebea84d-2daa-4428-ae99-5a69b7591e6c",
   "metadata": {},
   "source": [
    "Creating dataset of positive pairs. I combine questions with answers and select, for example, 4000 random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860bf171-35ff-4afa-a370-3facb6580214",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = qrels.merge(queries, left_on=\"query-id\", right_on=\"_id\").merge(corpus, left_on=\"corpus-id\", right_on=\"_id\")\n",
    "positives = positives[['text_x', 'text_y', 'score']].rename(columns={'text_x': 'query', 'text_y': 'answer'}).sample(n=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e385a51-df81-4296-a457-a45a228f892f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13567</th>\n",
       "      <td>Handel poufny powiƒÖzanym papierem warto≈õciowym...</td>\n",
       "      <td>Je≈õli jeste≈õ w stanie uzyskaƒá informacje, kt√≥r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4034</th>\n",
       "      <td>Po≈ºyczanie pieniƒôdzy, a nastƒôpnie ich inwestow...</td>\n",
       "      <td>Wykorzystam 10% z tych 20 000 na sp≈Çatƒô po≈ºycz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11617</th>\n",
       "      <td>Podatki od sprzeda≈ºy akcji</td>\n",
       "      <td>@BlackJack robi dobrƒÖ odpowied≈∫ na temat zysk√≥...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   query  \\\n",
       "13567  Handel poufny powiƒÖzanym papierem warto≈õciowym...   \n",
       "4034   Po≈ºyczanie pieniƒôdzy, a nastƒôpnie ich inwestow...   \n",
       "11617                         Podatki od sprzeda≈ºy akcji   \n",
       "\n",
       "                                                  answer  score  \n",
       "13567  Je≈õli jeste≈õ w stanie uzyskaƒá informacje, kt√≥r...      1  \n",
       "4034   Wykorzystam 10% z tych 20 000 na sp≈Çatƒô po≈ºycz...      1  \n",
       "11617  @BlackJack robi dobrƒÖ odpowied≈∫ na temat zysk√≥...      1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e9959-1414-4aca-9ba7-2a2f29afdd4d",
   "metadata": {},
   "source": [
    "I am creating a dictionary of correct answers to questions, so that when drawing nagative answers, I can check for positive ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "334d9d83-39d0-49a9-b963-a1725bd4a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dict = {}\n",
    "\n",
    "for row in ds_qa:\n",
    "    query_id = row['query-id']\n",
    "    corpus_id = row['corpus-id']\n",
    "    \n",
    "    if corpus_id not in qa_dict:\n",
    "        qa_dict[corpus_id] = []\n",
    "    qa_dict[corpus_id].append(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a64575d-2537-4fbf-93ed-8b4ef5c51d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = {corpus_id: corpus[corpus['_id'] == corpus_id]['text'].values[0] for corpus_id in corpus['_id']}\n",
    "queries_dict = {query_id: queries[queries['_id'] == query_id]['text'].values[0] for query_id in queries['_id']}\n",
    "qa_queries = set(ds_qa['query-id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88270498-8be4-48b7-a866-37ce94474ff2",
   "metadata": {},
   "source": [
    "I choose a random text from all corpus_id. Then I draw some question for it from ds_qa (that is, from those questions that have some correct answer).  \n",
    "I draw 12,000 rows so that the ratio of positive examples to negative ones is 1:3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84108ee9-6c7e-41b1-95b2-414a858c29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives_rows = []\n",
    "\n",
    "for i in range(12000):\n",
    "    correct = -1\n",
    "    corpus_id = random.choice(list(corpus['_id']))\n",
    "    if corpus_id in qa_dict:\n",
    "        correct = qa_dict[corpus_id]\n",
    "\n",
    "    query = random.choice(list(qa_queries))\n",
    "    while query == correct: # if answer is correct, draw again\n",
    "        query = random.choice(list(qa_queries))\n",
    "\n",
    "    row = {\n",
    "        'query': queries_dict.get(query, ''),\n",
    "        'answer': corpus_dict.get(corpus_id, ''),\n",
    "        'score': 0\n",
    "    }\n",
    "\n",
    "    negatives_rows.append(row)\n",
    "\n",
    "negatives = pd.DataFrame(negatives_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a34ca0-3acf-471d-b693-55e930489cac",
   "metadata": {},
   "source": [
    "Preparing a dataset as a collection of strings: {question} {separator} {passage}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a101d9c3-cbc2-4c52-b1b0-c45fa3987230",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([positives, negatives], ignore_index=True)\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "separator_token = tokenizer.sep_token\n",
    "dataset['pair'] = dataset.apply(lambda row: f\"{row['query']} {separator_token} {row['answer']}\", axis=1)\n",
    "\n",
    "dataset.drop(columns=['query', 'answer'], inplace=True)\n",
    "dataset = dataset[['pair','score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f33b308-ceaa-4579-bafd-61d2ce660f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dlaczego transakcje bankowe nie sƒÖ natychmiast...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kr√≥tka koncepcja ruchu ceny okre≈õlonej akcji [...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Budowanie niezale≈ºno≈õci finansowej &lt;/s&gt; Wa≈ºne ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jak p√≥≈∫no mogƒô wp≈Çaciƒá pieniƒÖdze na konto IRA ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Czy jako obywatel niebƒôdƒÖcy obywatelem Indii m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Czy ≈õledzenie pieniƒôdzy i posiadanie bud≈ºetu t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Czy istniejƒÖ karty kredytowe z okresem wyciƒÖgu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dlaczego nigdy nie widzia≈Çem podzia≈Çu akcji? &lt;...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kiedy zlecenia stop/limit sƒÖ widoczne na otwar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Przyznanie akcji, podatki i IRS &lt;/s&gt; ‚ÄûZawsze m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                pair  score\n",
       "0  Dlaczego transakcje bankowe nie sƒÖ natychmiast...      1\n",
       "1  Kr√≥tka koncepcja ruchu ceny okre≈õlonej akcji [...      0\n",
       "2  Budowanie niezale≈ºno≈õci finansowej </s> Wa≈ºne ...      1\n",
       "3  jak p√≥≈∫no mogƒô wp≈Çaciƒá pieniƒÖdze na konto IRA ...      1\n",
       "4  Czy jako obywatel niebƒôdƒÖcy obywatelem Indii m...      0\n",
       "5  Czy ≈õledzenie pieniƒôdzy i posiadanie bud≈ºetu t...      1\n",
       "6  Czy istniejƒÖ karty kredytowe z okresem wyciƒÖgu...      0\n",
       "7  Dlaczego nigdy nie widzia≈Çem podzia≈Çu akcji? <...      1\n",
       "8  Kiedy zlecenia stop/limit sƒÖ widoczne na otwar...      0\n",
       "9  Przyznanie akcji, podatki i IRS </s> ‚ÄûZawsze m...      0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea352d-a14b-4462-b31c-29f1d9656a53",
   "metadata": {},
   "source": [
    "Splitting dataset with stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69df052d-e367-4d46-9470-f885e0b7e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(dataset, test_size=0.2, stratify=dataset['score'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['score'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "952171d8-48a5-4e64-bd60-aa3aacd5d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = train_df['pair'].tolist()\n",
    "val_pairs = val_df['pair'].tolist()\n",
    "test_pairs = test_df['pair'].tolist()\n",
    "\n",
    "train_labels = train_df['score'].tolist()\n",
    "val_labels = val_df['score'].tolist()\n",
    "test_labels = test_df['score'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be321f2f-2b28-496d-b5ac-8a274be1ebb3",
   "metadata": {},
   "source": [
    "Tokenization of data, because this is what the model works on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bff3646-9cdc-4079-9780-7e90066f39c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(pairs, labels):\n",
    "    encoding = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoding['labels'] = torch.tensor(labels)\n",
    "    return encoding\n",
    "\n",
    "train_encodings = tokenize_data(train_pairs, train_labels)\n",
    "val_encodings = tokenize_data(val_pairs, val_labels)\n",
    "test_encodings = tokenize_data(test_pairs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a089eb0a-b504-4fc7-b118-4dd54c99e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextPairDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65927f36-9f1d-4f01-8570-69a1673ab268",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextPairDataset(train_encodings)\n",
    "val_dataset = TextPairDataset(val_encodings)\n",
    "test_dataset = TextPairDataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fce86d1e-15cf-48cd-8978-995598bd69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import accelerate\n",
    "# print(\"Transformers version:\", transformers.__version__)\n",
    "# print(\"Accelerate version:\", accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b9a7d8f-b9bc-4a44-b428-d8da417f6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d4b69-7a25-40da-8254-dc68280dbcc1",
   "metadata": {},
   "source": [
    "During training, we calculate various metrics, but when selecting the best model, we rely on the F1 score (as it works well for imbalanced datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f621c958-e6fc-4ffc-954f-0d30f4104bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1) # converitng results to labels\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\") #!\n",
    "\n",
    "    return {\n",
    "        \"eval_accuracy\": accuracy,\n",
    "        \"eval_precision\": precision,\n",
    "        \"eval_recall\": recall,\n",
    "        \"eval_f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8294ba18-de72-4d9d-a444-62c54efaa7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available: True\n",
      "GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"Is GPU available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59af9f92-0c2d-42ea-b2e9-af1cddafd9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,  # Loading best model at the end due to f1-score\n",
    "    metric_for_best_model=\"eval_f1\", \n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    run_name=\"lab5_run\",\n",
    "    logging_steps=30,\n",
    "    save_strategy=\"epoch\",  # Record every epoch, just like an evaluation\n",
    "    learning_rate=5e-05,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221090c-daff-41aa-8a0e-1e61fd392b56",
   "metadata": {},
   "source": [
    "Training was commented out because the model was trained on Google Colab using a more powerful GPU, then saved to a computer from which the trained model is now being loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f7afcd10-945f-4470-8c33-5d45c876fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b29db950-8686-4f6a-b17d-5f77c4979887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "client = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", \"CzemuTakieDlugie24\"),\n",
    "    verify_certs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ecaaa-321c-4fd8-8aa4-e8339ad98e92",
   "metadata": {},
   "source": [
    "Not pretty code but I use code from lab2 that worked for 4 analyzers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f02a0f8a-8783-456f-bd9f-b7f541cf109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_text(index_name: str, queries: dict, k: int):\n",
    "\n",
    "    model_results = [[] for _ in range(1)] \n",
    "    \n",
    "    for query_id, query_text in queries.items():\n",
    "        search_bodies = [\n",
    "            {\n",
    "                \"query\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query_text,\n",
    "                        \"fields\": [\"without_synonyms_with_lemmatizer\"]\n",
    "                    }\n",
    "                },\n",
    "                \"size\": k\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        for i, search_body in enumerate(search_bodies):\n",
    "            response = client.search(index=index_name, body=search_body)\n",
    "            \n",
    "            for hit in response['hits']['hits']:\n",
    "                model_results[i].append({\n",
    "                    'query-id': query_id,\n",
    "                    'corpus-id': int(hit['_id'])\n",
    "                })\n",
    "\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac11c827-8f8d-4851-9cf2-a62a9af982e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "ds_qa = load_dataset(\"clarin-knext/fiqa-pl-qrels\")['test']  # Testing model and calculating NDCG on TEST data\n",
    "test_query_ids = set(ds_qa['query-id'])\n",
    "filtered_queries = OrderedDict(sorted((int(row['_id']), row['text']) for row in ds_queries['queries'] if int(row['_id']) in test_query_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8fcf245e-1cbb-402c-ae7d-09a095e91978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'query-id': 8, 'corpus-id': 309023},\n",
       " {'query-id': 8, 'corpus-id': 65404},\n",
       " {'query-id': 8, 'corpus-id': 438975}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = search_text(\"fiqa_pl_index_v3\", filtered_queries, 30)[0] # we search top 30 texts for each query from the test set\n",
    "print(len(results))\n",
    "results[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc8996-7489-4c0c-9aa3-1958c392eea4",
   "metadata": {},
   "source": [
    "### Loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "adfa4d9d-6460-4477-9008-f33298309f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['config.json', 'merges.txt', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json']\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\karol\\Desktop\\STUDIA\\pjn\\my_model\"\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8c41d8d-d95a-40a3-8c34-861151112835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                   # Model do wytrenowania\n",
    "    args=training_args,            # Argumenty trenowania\n",
    "    train_dataset=train_dataset,   # Zbi√≥r danych treningowych\n",
    "    eval_dataset=val_dataset,       # Zbi√≥r danych walidacyjnych\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d94d64-32c4-44d6-9cc5-edd1bca7a19b",
   "metadata": {},
   "source": [
    "Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08f0c840-d507-493d-90f7-fbf888f8040c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.953125,\n",
       " 'eval_precision': 0.9529909490256199,\n",
       " 'eval_recall': 0.953125,\n",
       " 'eval_f1': 0.9530511832718206,\n",
       " 'eval_loss': 0.17457541823387146,\n",
       " 'eval_model_preparation_time': 0.0035,\n",
       " 'eval_runtime': 24.8192,\n",
       " 'eval_samples_per_second': 103.146,\n",
       " 'eval_steps_per_second': 3.223}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8762e1ac-82e7-438c-8f15-58b009b78e65",
   "metadata": {},
   "source": [
    "Here I convert the questions from the qa_test set into the format expected by the model, which is query < / s > text. Then, I tokenize them, as the model requires tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02cb0c81-b652-4adc-be86-4ad5fa8de5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19440"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_to_classify = []\n",
    "\n",
    "for result in results:\n",
    "    first_part = filtered_queries[result['query-id']] + \" </s>\"\n",
    "    second_part = corpus_dict[result['corpus-id']]\n",
    "    pair = first_part + second_part\n",
    "    pairs_to_classify.append(pair)\n",
    "\n",
    "pairs_tokenized = [tokenizer(pair, return_tensors=\"pt\", truncation=True).to(\"cuda\") for pair in pairs_to_classify]\n",
    "len(pairs_to_classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f4038-15d2-4b93-913d-f07c071512d8",
   "metadata": {},
   "source": [
    "I create list of tuples (query_id, corpus_id) to save the prediction results as a dictionary: (query_id, corpus_id): (predicted_class, probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36cae6c6-45f8-4dc6-be00-085c081f196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tuple = [(d['query-id'], d['corpus-id']) for d in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e39d9-7176-4d1a-b602-59288a74277e",
   "metadata": {},
   "source": [
    "Here we make predictions: for each tokenized example, we calculate the class and the probability with which the model is confident. We save this to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "acd63380-ee7e-4bd6-aa67-dbd64fc7ff56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "\n",
    "for i, inputs in enumerate(pairs_tokenized):\n",
    "\n",
    "    if i % 1000 == 0: # just tracking how much is left\n",
    "        print(i)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "        positive_class_prob = probabilities[0][1].item() \n",
    "\n",
    "    predictions[results_tuple[i]] = (predicted_class, positive_class_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d645d-55f5-4c32-973d-aa8577f6080b",
   "metadata": {},
   "source": [
    "I sort against prediction probability by grouping by each query_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "42db7654-406c-4318-90e9-5af05705b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sorted = dict(\n",
    "    sorted(\n",
    "        predictions.items(),\n",
    "        key=lambda item: (item[0][0], -item[1][1])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf6e7df-0f17-4ac5-bf18-a043be903131",
   "metadata": {},
   "source": [
    "Then from each group we leave only the ‚Äúk‚Äù most certain results (5 in our case because we'll be calculating NDCG5 as in lab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6934641a-29ec-41ac-b67b-4bc1eabd0543",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "grouped = defaultdict(list)\n",
    "for key, value in pred_sorted.items():\n",
    "    grouped[key[0]].append((key, value))\n",
    "\n",
    "# Zachowanie tylko 5 pierwszych element√≥w z ka≈ºdej grupy\n",
    "filtered_data = {key: v[:k] for key, v in grouped.items()}\n",
    "\n",
    "# Z≈ÇƒÖczenie wynik√≥w z powrotem w jeden s≈Çownik\n",
    "pred = {key: value for group in filtered_data.values() for key, value in group}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db09c5a6-6cda-4213-a042-fd7a1170bb88",
   "metadata": {},
   "source": [
    "Then I format the data to the same format as in lab2 to use the same code (we have: [[ ]]) because in lab 2 there were 4 analyzers, here we give 1 but for code consistency also give a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e62d4a13-73dc-4175-ae50-51ff9c5d88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_pred = [[{'query-id': query_id, 'corpus-id': corpus_id, 'analyzer': 0} for (query_id, corpus_id), (anayzer, _) in pred.items()]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b9a85-ec55-43de-a1d0-16f13ec3b0dc",
   "metadata": {},
   "source": [
    "Creating a list containing valid results (those from the ds_qa dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "709f5a2d-bda9-4b28-8927-265feccf51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_results = defaultdict(list)\n",
    "\n",
    "for entry in ds_qa:\n",
    "    query_id = entry['query-id']\n",
    "    corpus_id = entry['corpus-id']\n",
    "    correct_results[query_id].append(corpus_id)\n",
    "\n",
    "correct_list = [{'query-id': query_id, 'corpuses-id': corpuses} for query_id, corpuses in correct_results.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e8fb05d0-c16a-4066-a984-39c5f81563b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DCG(relevance_scores, k):\n",
    "    return sum(rel / np.log2(idx + 2) for idx, rel in enumerate(relevance_scores[:k]))\n",
    "\n",
    "def get_NDCG(model_results, relevant_documents, k):\n",
    "\n",
    "    relevance_scores = [1 if doc['corpus-id'] in relevant_documents else 0 for doc in model_results]\n",
    "    dcg_k = get_DCG(relevance_scores, k)\n",
    "\n",
    "    num_docs = len(relevant_documents)\n",
    "    ideal_relevance_scores = [1] * min(num_docs, k) + [0] * (k - min(num_docs, k))\n",
    "    idcg_k = get_DCG(ideal_relevance_scores, k)\n",
    "\n",
    "    ndcg_k = dcg_k / idcg_k if idcg_k > 0 else 0\n",
    "    return ndcg_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d3efa2a3-caa9-4ca1-be8c-a94fef7b34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgs = []\n",
    "k=5\n",
    "\n",
    "for j in range(len(formatted_pred)):\n",
    "    results = []\n",
    "    for i in range(len(correct_list)):\n",
    "        ndcg = get_NDCG(formatted_pred[j][i*k:(i+1)*k], correct_list[i]['corpuses-id'], k)\n",
    "        results.append((correct_list[i]['query-id'], float(ndcg)))\n",
    "\n",
    "    ndcgs.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5bbdb9f9-3c4b-4aa9-a9ec-dc57a4cf84cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result after training: 21.45 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ndcgs)):\n",
    "    avg = sum(value for _, value in ndcgs[i]) / len(ndcgs[i])\n",
    "    print(f\"Result after training: {round(avg*100,2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afad7de-106e-4fb6-9607-6fe97248e8b8",
   "metadata": {},
   "source": [
    "In Lab2, the result for the same dataset and analyzer was 18.51%. With the current approach, we achieved a 3% improvement, bringing the result up to 21.45%. While this represents a noticeable increase, the overall difference is relatively modest. Nevertheless, it is still a significant improvement in the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc1090-9d71-4c7d-9cde-deb1df935eeb",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c78e4b-e1b9-4b04-a642-7a13071ba989",
   "metadata": {},
   "source": [
    "### 1) Do you think simpler methods, like Bayesian bag-of-words model, would work for sentence-pair classification? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb77f60f-7c4a-4d05-a9a5-ea89bc2d97d5",
   "metadata": {},
   "source": [
    "Although Bayesian and bag-of-words models can be used for sentence-pair classification, they generally lack the ability to capture the nuanced semantic relationships and contextual dependencies required for such tasks. For example, the bag-of-words model treats words in isolation, ignoring their order and interdependencies. For this reason, these models are not ideal and would likely yield subpar results compared to modern NLP models that are explicitly designed for such tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf5c50-0778-46a5-8a57-8aef99b28417",
   "metadata": {},
   "source": [
    "### 2) What hyper-parameters you have selected for the training? What resources (papers, tutorial) you have consulted to select these hyper-parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c65442-1355-4373-b639-b0575397ee72",
   "metadata": {},
   "source": [
    "For the chosen hyperparameters, I decided on the following values:\n",
    "\n",
    "learning_rate=5e-05 ‚Äì This is one of the default values used in this type of problem; based on discussions I found on forums, it seems to perform well.   \n",
    "batch_size=32 ‚Äì I experimented with various values, but for this batch size, I found a good balance between training stability, model generalization, and training speed.   \n",
    "epochs=5 ‚Äì I conducted several experiments and didn't observe a significant improvement in results from one epoch to the next, so I decided to limit the number of epochs to five, \n",
    " especially since the model already took a considerable amount of time to train.   \n",
    "fp16 ‚Äì After researching, I found that using 16-bit floating point precision can speed up training, especially on certain GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae62457-8914-462f-a54a-34092be81851",
   "metadata": {},
   "source": [
    "### 3) Think about pros and cons of the neural-network models with respect to natural language processing. Provide at least 2 pros and 2 cons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a790df00-6105-487e-9241-3ea48a17c4c9",
   "metadata": {},
   "source": [
    "#### Pros: \n",
    "- Neural networks, especially deep learning models like transformers (e.g., BERT, GPT), excel at learning complex and subtle patterns in data. They can capture long-range dependencies in text, such as context between words that are far apart in a sentence, which simpler models like bag-of-words or rule-based systems cannot.\n",
    "- Neural network models can be fine-tuned for specific tasks with relatively small amounts of labeled data, thanks to techniques like transfer learning. Models such as BERT and GPT can be adapted to new tasks with excellent performance even with limited task-specific data. This greatly reduces the need for extensive labeled datasets.\n",
    "\n",
    "#### Cons\n",
    "- Training deep neural networks requires significant computational resources, including powerful GPUs or TPUs and considerable time. Additionally, fine-tuning large pre-trained models for specific tasks can still be resource-intensive, even though it requires less data.\n",
    "- For neural netowrks it is difficult to understand how they make decisions. This lack of transparency can be problematic, especially in sensitive domains like healthcare, law, or finance, where understanding the rationale behind a model's prediction is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c1749-3670-488e-b5d9-93d033540ed0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
